[하둡] 자동화와 [스파크] 자동화 설치 방법
[hadoop 설치]----------------------------------------------------------------------------
0a. docker-compose.yml 파일위치에서 docker compose up -d 로
   namenode , datanode1 , datanode2 , datanode3 생성한다.
0b.  ssh hadoop@localhost -p 2220 에 접속한다.
0c. namenode 에서 ssh-keygen -t rsa 를 이용하여 공개키 생성후
0d. 각노드로 복사해줌. ssh-copy-id hadoop@namenode , datanode1,2,3 총 4군데
0e. namenode에  myset.sh 복사후  실행 -> 각노드에 자동으로 hadoop가 설치됨

[spark 설치]----------------------------------------------------------------------------
datanode1에는 spark를 설치해야함.
이유는 통상 namenode 에 work 및 master를 설치를 자제함,
복잡성, 관리성, namenode충돌피하려고... 
그래서 스파크는 myspark3g.sh 를 실행하는곳이 datanode1이 되어야 하고
datanode1 - master , worker1
datanode2 -  worker2
datanode3 -  worker3
이렇게 설치되도록
ssh 로 datanode1에 접속하여  myspark3g.sh 를 실행한다.

[설치완료 후]----------------------------------------------------------------------------
실행 방법 디테일하게
1) namenode 에서 hadoop 실행
  . start-all.sh

그후 에러잡기위해서 아래두개(이유는 hdfs log폴더가 있어야 스파크가 실행됨)
hdfs dfs -mkdir -p /spark-logs
hdfs dfs -chmod -R 1777 /spark-logs

2) datanode1 에서 spark 실행 ,
  .  /home/hadoop/spark/sbin/start-all.sh  
   (스파크 실행 경로를 쓰는 이유는 hadoop의 실행방법인 start-all.sh 와 이름이 같아서 다르게 실행해야함)

spark-shell





