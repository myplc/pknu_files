# !pip install selenium
# !pip install pandas


from selenium import webdriver
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from datetime import datetime
from bs4 import BeautifulSoup as bs
import pandas as pd
import time as t
from urllib.parse import quote


import json
file_path = '숙박소리스트.json'

with open(file_path, 'r', encoding='utf-8') as f:
    stay_list = json.load(f)  # 리스트로 로딩됨
print(stay_list[:5],len(stay_list))


# Chrome 옵션 설정
options = Options()
options.binary_location = "..\\chrome-win64\\chrome.exe" # chrome 경로 설정
options.add_argument("--headless=new") # 헤드리스
options.add_argument("--no-sandbox") # sandbox 설정
options.add_argument("--disable-dev-shm-usage") # 공유메모리 설정
options.add_argument("--window-size=1920,1080") # 화면크기설정 , 반응형 웹 대비
# options.add_argument("--disable-gpu") # GPU 비활성화

# ChromeDriver 설정
service = Service("..\\chromedriver.exe")
driver = webdriver.Chrome(service=service, options=options)

url_array = []
h_list = stay_list

def scan_url(want):
    eurl = quote(want+" 부산")
    turl = f"https://www.bing.com/images/search?q={eurl}&go=검색&qs=ds&form=QBIR&first=1"
    driver.get(turl)
    print(driver.title)
    
    # 요소찾기
    t.sleep(0.8)
    try:
        element = driver.find_element(By.CSS_SELECTOR, "#mmComponent_images_2 img")
        img_url = element.get_attribute('src')
    except NoSuchElementException:
        img_url = 'xx링크없음수동필요xx'  # 요소가 없으면 기본값 넣기

    # 출력 및 저장
    url_array.append(img_url)

n = 0
for v in h_list:    
    scan_url(v)
    t.sleep(3.1)
    n +=1
    print(f"{n}차 진행중")

print(url_array)
driver.quit() # 브라우저 종료
# URL 리스트를 텍스트 파일로 저장
with open("image_urls2.txt", "w", encoding="utf-8") as f:
    for url in url_array:
        f.write(url + "\n")


url_array


# 현재 페이지 정보 가져오기
star_list=[]
soup = bs(html, 'html.parser')
# 본문내용 가져오기
contents = soup.select("#mCSB_3_container > ul > li")

for c in contents:
    result = [ c['data-name'], c['data-lat'], c['data-long'], c.select_one('p.result_details').text , c.select_one('i')['class'][0] ]
    star_list.append(result)    
print(star_list)


# star_list


columns = ['이름','위도','경도','주소','타입']
sb = pd.DataFrame(star_list , columns=columns)
sb.head()


sb.info()


timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
csv_path = f'starbucks_{timestamp}.csv'
sb.to_csv(csv_path, index=True)
print("csv 파일 생성완료.")


# !pip install sqlalchemy
# !pip install mysql-connector-python


# mysql 에서 데이터 가져와 dataframe 로 변환하기
import mysql.connector
from sqlalchemy import create_engine

user = "root"
password = "1234"
host = "210.119.14.56"
port = "3306"
database = "jh0521"

total = f"{user}:{password}@{host}:{port}/{database}"
print(total)
engine = create_engine('mysql+mysqlconnector://'+total)

query = "SELECT * FROM STAR_STORE" # 테이블이름 주의
df = pd.read_sql(query, engine)
df.head()


df = df[['storename','latitude','longitude','location','type']]
df.head()


df.columns =  ['이름','위도','경도','주소','타입']
df.head()



